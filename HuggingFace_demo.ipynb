{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  9 21:16:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   46C    P4             10W /   55W |     866MiB /   8188MiB |     38%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3584      G   /usr/lib/xorg/Xorg                            312MiB |\n",
      "|    0   N/A  N/A      3959      G   /usr/bin/gnome-shell                           95MiB |\n",
      "|    0   N/A  N/A     14171      G   /usr/bin/gnome-text-editor                     43MiB |\n",
      "|    0   N/A  N/A    619841      G   ...39,262144 --variations-seed-version        232MiB |\n",
      "|    0   N/A  N/A    632695      G   ...erProcess --variations-seed-version        166MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ballaprr/pytorch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-09 21:16:17.597253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 21:16:17.793480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746850577.890456  633369 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746850577.919211  633369 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746850578.101141  633369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746850578.101189  633369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746850578.101196  633369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746850578.101200  633369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 21:16:18.125511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "'''\n",
    "1. Text Classification: Assigning a category to a pice of text.\n",
    "Sentiment Analysis\n",
    "Topic Classification\n",
    "Spam Detection\n",
    "'''\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "\n",
    "'''\n",
    "2. Token Classification: Assigning labels to individual tokens in a sequence.\n",
    "Named Entity Recognition (NER)\n",
    "Part-of-Speech Tagging\n",
    "'''\n",
    "\n",
    "token_classifier = pipeline('token-classification')\n",
    "\n",
    "'''\n",
    "3. Question Answering: Extracting an answer from a given context based on a question.\n",
    "'''\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "'''\n",
    "4. Text Generation: Generating text based on a given prompt.\n",
    "Language Modeling\n",
    "Story Generation\n",
    "\n",
    "'''\n",
    "\n",
    "text_generator = pipeline('text-generation')\n",
    "\n",
    "'''\n",
    "5. Summarization: Condensing a longer text into a shorter summary.\n",
    "'''\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.999748170375824}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"I was not happy with the last Mission Impossible Movie\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995480179786682}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis')(\"I was confused with the Barie Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9924917817115784}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis')\\\n",
    "    (\"Everyday lots of LLMs paper are published about LLMs Evaluation. \\\n",
    "    Lots of them Looks very Promising \\\n",
    "    I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "    There is still lots to do. \\\n",
    "    Don't you think?\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.840279757976532}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis', model = 'facebook/bart-large-mnli')\\\n",
    "                                    (\"Everyday lots of LLMs papers are published about LLMs Evaluation. \\\n",
    "                                    Lots of them Looks very Promising. \\\n",
    "                                    I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "                                    There is still lots to do. \\\n",
    "                                    Don't you think?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9978686571121216}\n",
      "{'label': 'NEGATIVE', 'score': 0.9995476603507996}\n",
      "{'label': 'NEGATIVE', 'score': 0.9983084201812744}\n",
      "{'label': 'NEGATIVE', 'score': 0.9960630536079407}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task = 'sentiment-analysis')\n",
    "\n",
    "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
    "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
    "            \"PassiveAgressive is the name of a Linear Regression model that so many people do not know.\", \\\n",
    "            \"I hate long Meetings\"]\n",
    "\n",
    "results = classifier(task_list)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'admiration', 'score': 0.7406538128852844}\n",
      "{'label': 'confusion', 'score': 0.9066852331161499}\n",
      "{'label': 'neutral', 'score': 0.79802405834198}\n",
      "{'label': 'anger', 'score': 0.7903208136558533}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task = \"sentiment-analysis\", model = \"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
    "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
    "            \"PassiveAgressive is the name of a Linear Regression model that so many people do not know.\", \\\n",
    "            \"I hate long Meetings\"]\n",
    "\n",
    "results = classifier(task_list)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Today is a rainy day in London to be celebrated by people with a good reason.\n",
      "\n",
      "\n",
      "\n",
      "The most common day for you is 6am.\n",
      "Today, we're going to celebrate our very first day we are going to go ahead without\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline('text-generation', model=\"distilbert/distilgpt2\")\n",
    "generated_text = text_generator(\"Today is a rainy day in London\", \\\n",
    "                                truncation=True,\n",
    "                                num_return_sequences=2)\n",
    "\n",
    "print(\"Generated Text: \", generated_text[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7823826670646667,\n",
       " 'start': 5,\n",
       " 'end': 25,\n",
       " 'answer': 'developing AI models'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline('question-answering')\n",
    "question = \"What is my job?\"\n",
    "context = \"I am developing AI models with Python.\"\n",
    "\n",
    "qa_model(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '2 stars', 'score': 0.5077593326568604}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\"\"\"\n",
    "model_name1 = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "mytokenizer1 = DistilBertTokenizer.from_pretrained(model_name1)\n",
    "mymodel1 = DistilBertForSequenceClassification.from_pretrained(model_name1)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", )\n",
    "res = classifier(\"I was so not happy with the Barie Movie\")\n",
    "print(res)\n",
    "\"\"\"\n",
    "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "mymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
    "mytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=mymodel2, tokenizer=mytokenizer2)\n",
    "res = classifier(\"I was so not happy with the Barie Movie\")\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text:  ['i', 'was', 'so', 'not', 'happy', 'with', 'the', 'barbie', 'movie']\n",
      "Token IDs:  [1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185]\n",
      "Encoded Inpit: {'input_ids': [101, 1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"I was so not happy with the Barbie Movie\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the tokenized text and IDs\n",
    "print(\"Tokenized Text: \", tokens)\n",
    "print(\"Token IDs: \", input_ids)\n",
    "\n",
    "# Encode the text (tokenization + converting to input IDs)\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"Encoded Inpit:\", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input: {'input_ids': [101, 1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Encode the text (tokenization + converting to input IDs)\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"Encoded Input:\", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode Output: i was so not happy with the barbie movie\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(input_ids)\n",
    "print(\"Decode Output:\", decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Install Necessary Libraries\n",
    "##### Step 2: Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Preprocess the Data\n",
    "##### Tokenize the dataset using the tokenizer associated with the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:09<00:00, 2623.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  12524,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2013,\n",
       "  2026,\n",
       "  2678,\n",
       "  3573,\n",
       "  2138,\n",
       "  1997,\n",
       "  2035,\n",
       "  1996,\n",
       "  6704,\n",
       "  2008,\n",
       "  5129,\n",
       "  2009,\n",
       "  2043,\n",
       "  2009,\n",
       "  2001,\n",
       "  2034,\n",
       "  2207,\n",
       "  1999,\n",
       "  3476,\n",
       "  1012,\n",
       "  1045,\n",
       "  2036,\n",
       "  2657,\n",
       "  2008,\n",
       "  2012,\n",
       "  2034,\n",
       "  2009,\n",
       "  2001,\n",
       "  8243,\n",
       "  2011,\n",
       "  1057,\n",
       "  1012,\n",
       "  1055,\n",
       "  1012,\n",
       "  8205,\n",
       "  2065,\n",
       "  2009,\n",
       "  2412,\n",
       "  2699,\n",
       "  2000,\n",
       "  4607,\n",
       "  2023,\n",
       "  2406,\n",
       "  1010,\n",
       "  3568,\n",
       "  2108,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  3152,\n",
       "  2641,\n",
       "  1000,\n",
       "  6801,\n",
       "  1000,\n",
       "  1045,\n",
       "  2428,\n",
       "  2018,\n",
       "  2000,\n",
       "  2156,\n",
       "  2023,\n",
       "  2005,\n",
       "  2870,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5436,\n",
       "  2003,\n",
       "  8857,\n",
       "  2105,\n",
       "  1037,\n",
       "  2402,\n",
       "  4467,\n",
       "  3689,\n",
       "  3076,\n",
       "  2315,\n",
       "  14229,\n",
       "  2040,\n",
       "  4122,\n",
       "  2000,\n",
       "  4553,\n",
       "  2673,\n",
       "  2016,\n",
       "  2064,\n",
       "  2055,\n",
       "  2166,\n",
       "  1012,\n",
       "  1999,\n",
       "  3327,\n",
       "  2016,\n",
       "  4122,\n",
       "  2000,\n",
       "  3579,\n",
       "  2014,\n",
       "  3086,\n",
       "  2015,\n",
       "  2000,\n",
       "  2437,\n",
       "  2070,\n",
       "  4066,\n",
       "  1997,\n",
       "  4516,\n",
       "  2006,\n",
       "  2054,\n",
       "  1996,\n",
       "  2779,\n",
       "  25430,\n",
       "  14728,\n",
       "  2245,\n",
       "  2055,\n",
       "  3056,\n",
       "  2576,\n",
       "  3314,\n",
       "  2107,\n",
       "  2004,\n",
       "  1996,\n",
       "  5148,\n",
       "  2162,\n",
       "  1998,\n",
       "  2679,\n",
       "  3314,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1012,\n",
       "  1999,\n",
       "  2090,\n",
       "  4851,\n",
       "  8801,\n",
       "  1998,\n",
       "  6623,\n",
       "  7939,\n",
       "  4697,\n",
       "  3619,\n",
       "  1997,\n",
       "  8947,\n",
       "  2055,\n",
       "  2037,\n",
       "  10740,\n",
       "  2006,\n",
       "  4331,\n",
       "  1010,\n",
       "  2016,\n",
       "  2038,\n",
       "  3348,\n",
       "  2007,\n",
       "  2014,\n",
       "  3689,\n",
       "  3836,\n",
       "  1010,\n",
       "  19846,\n",
       "  1010,\n",
       "  1998,\n",
       "  2496,\n",
       "  2273,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  8563,\n",
       "  2033,\n",
       "  2055,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  2008,\n",
       "  2871,\n",
       "  2086,\n",
       "  3283,\n",
       "  1010,\n",
       "  2023,\n",
       "  2001,\n",
       "  2641,\n",
       "  26932,\n",
       "  1012,\n",
       "  2428,\n",
       "  1010,\n",
       "  1996,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  5019,\n",
       "  2024,\n",
       "  2261,\n",
       "  1998,\n",
       "  2521,\n",
       "  2090,\n",
       "  1010,\n",
       "  2130,\n",
       "  2059,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2915,\n",
       "  2066,\n",
       "  2070,\n",
       "  10036,\n",
       "  2135,\n",
       "  2081,\n",
       "  22555,\n",
       "  2080,\n",
       "  1012,\n",
       "  2096,\n",
       "  2026,\n",
       "  2406,\n",
       "  3549,\n",
       "  2568,\n",
       "  2424,\n",
       "  2009,\n",
       "  16880,\n",
       "  1010,\n",
       "  1999,\n",
       "  4507,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  2024,\n",
       "  1037,\n",
       "  2350,\n",
       "  18785,\n",
       "  1999,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2130,\n",
       "  13749,\n",
       "  7849,\n",
       "  24544,\n",
       "  1010,\n",
       "  15835,\n",
       "  2037,\n",
       "  3437,\n",
       "  2000,\n",
       "  2204,\n",
       "  2214,\n",
       "  2879,\n",
       "  2198,\n",
       "  4811,\n",
       "  1010,\n",
       "  2018,\n",
       "  3348,\n",
       "  5019,\n",
       "  1999,\n",
       "  2010,\n",
       "  3152,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1045,\n",
       "  2079,\n",
       "  4012,\n",
       "  3549,\n",
       "  2094,\n",
       "  1996,\n",
       "  16587,\n",
       "  2005,\n",
       "  1996,\n",
       "  2755,\n",
       "  2008,\n",
       "  2151,\n",
       "  3348,\n",
       "  3491,\n",
       "  1999,\n",
       "  1996,\n",
       "  2143,\n",
       "  2003,\n",
       "  3491,\n",
       "  2005,\n",
       "  6018,\n",
       "  5682,\n",
       "  2738,\n",
       "  2084,\n",
       "  2074,\n",
       "  2000,\n",
       "  5213,\n",
       "  2111,\n",
       "  1998,\n",
       "  2191,\n",
       "  2769,\n",
       "  2000,\n",
       "  2022,\n",
       "  3491,\n",
       "  1999,\n",
       "  26932,\n",
       "  12370,\n",
       "  1999,\n",
       "  2637,\n",
       "  1012,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  1037,\n",
       "  2204,\n",
       "  2143,\n",
       "  2005,\n",
       "  3087,\n",
       "  5782,\n",
       "  2000,\n",
       "  2817,\n",
       "  1996,\n",
       "  6240,\n",
       "  1998,\n",
       "  14629,\n",
       "  1006,\n",
       "  2053,\n",
       "  26136,\n",
       "  3832,\n",
       "  1007,\n",
       "  1997,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2021,\n",
       "  2428,\n",
       "  1010,\n",
       "  2023,\n",
       "  2143,\n",
       "  2987,\n",
       "  1005,\n",
       "  1056,\n",
       "  2031,\n",
       "  2172,\n",
       "  1997,\n",
       "  1037,\n",
       "  5436,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,        \n",
    "    per_device_eval_batch_size=4,        \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                            \n",
    "    dataloader_pin_memory=False,           \n",
    "    report_to=\"none\"                       \n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 48:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.698200</td>\n",
       "      <td>0.702150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.694092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.693115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.6975780224609375, metrics={'train_runtime': 2939.4433, 'train_samples_per_second': 25.515, 'train_steps_per_second': 6.379, 'total_flos': 1.9733329152e+16, 'train_loss': 0.6975780224609375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 02:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.693115234375, 'eval_runtime': 153.665, 'eval_samples_per_second': 162.692, 'eval_steps_per_second': 40.673, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-tokenizer/tokenizer_config.json',\n",
       " './fine-tuned-tokenizer/special_tokens_map.json',\n",
       " './fine-tuned-tokenizer/vocab.txt',\n",
       " './fine-tuned-tokenizer/added_tokens.json',\n",
       " './fine-tuned-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-tokenizer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633369/2575282182.py:5: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-08 17:59:54+00:00</td>\n",
       "      <td>3D Scene Generation: A Survey</td>\n",
       "      <td>3D scene generation seeks to synthesize spatially structured, semantically\\nmeaningful, and photorealistic environments for applications such as immersive\\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\\nprocedural rules offered scalability but limited diversity. Recent advances in\\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\\ndistributions, improving fidelity, diversity, and view consistency. Recent\\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\\nreframing generation as image or video synthesis problems. This survey provides\\na systematic overview of state-of-the-art approaches, organizing them into four\\nparadigms: procedural generation, neural 3D-based generation, image-based\\ngeneration, and video-based generation. We analyze their technical foundations,\\ntrade-offs, and representative results, and review commonly used datasets,\\nevaluation protocols, and downstream applications. We conclude by discussing\\nkey challenges in generation capacity, 3D representation, data and annotations,\\nand evaluation, and outline promising directions including higher fidelity,\\nphysics-aware and interactive generation, and unified perception-generation\\nmodels. This review organizes recent advances in 3D scene generation and\\nhighlights promising directions at the intersection of generative AI, 3D\\nvision, and embodied intelligence. To track ongoing developments, we maintain\\nan up-to-date project page:\\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-08 17:59:47+00:00</td>\n",
       "      <td>DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</td>\n",
       "      <td>Current Structure-from-Motion (SfM) methods typically follow a two-stage\\npipeline, combining learned or geometric pairwise reasoning with a subsequent\\nglobal optimization step. In contrast, we propose a data-driven multi-view\\nreasoning approach that directly infers 3D scene geometry and camera poses from\\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\\nand cameras as pixel-wise ray origins and endpoints in a global frame and\\nemploys a transformer-based denoising diffusion model to predict them from\\nmulti-view inputs. To address practical challenges in training diffusion models\\nwith missing data and unbounded scene coordinates, we introduce specialized\\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\\nboth synthetic and real datasets, demonstrating that it outperforms classical\\nand learning-based approaches while naturally modeling uncertainty.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-08 17:58:49+00:00</td>\n",
       "      <td>Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning</td>\n",
       "      <td>Leveraging current legal standards, we define bias through the lens of\\nmarginal benefits and objective testing with the novel metric \"Objective\\nFairness Index\". This index combines the contextual nuances of objective\\ntesting with metric stability, providing a legally consistent and reliable\\nmeasure. Utilizing the Objective Fairness Index, we provide fresh insights into\\nsensitive machine learning applications, such as COMPAS (recidivism\\nprediction), highlighting the metric's practical and theoretical significance.\\nThe Objective Fairness Index allows one to differentiate between discriminatory\\ntests and systemic disparities.</td>\n",
       "      <td>[cs.CY, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-08 17:58:45+00:00</td>\n",
       "      <td>Flow-GRPO: Training Flow Matching Models via Online RL</td>\n",
       "      <td>We propose Flow-GRPO, the first method integrating online reinforcement\\nlearning (RL) into flow matching models. Our approach uses two key strategies:\\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\\n(SDE) that matches the original model's marginal distribution at all timesteps,\\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\\nstrategy that reduces training denoising steps while retaining the original\\ninference timestep number, significantly improving sampling efficiency without\\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\\nperfect object counts, spatial relations, and fine-grained attributes, boosting\\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\\nFlow-GRPO also achieves substantial gains in human preference alignment.\\nNotably, little to no reward hacking occurred, meaning rewards did not increase\\nat the cost of image quality or diversity, and both remained stable in our\\nexperiments.</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-08 17:57:40+00:00</td>\n",
       "      <td>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</td>\n",
       "      <td>We present StreamBridge, a simple yet effective framework that seamlessly\\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\\nfundamental challenges in adapting existing models into online scenarios: (1)\\nlimited capability for multi-turn real-time understanding, and (2) lack of\\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\\nmemory buffer combined with a round-decayed compression strategy, supporting\\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\\nactivation model that can be effortlessly integrated into existing Video-LLMs,\\nenabling continuous proactive responses. To further support StreamBridge, we\\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\\nunderstanding, featuring interleaved video-text sequences and diverse\\ninstruction formats. Extensive experiments show that StreamBridge significantly\\nimproves the streaming understanding capabilities of offline Video-LLMs across\\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\\nstandard video understanding benchmarks.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.CL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-05-08 17:56:57+00:00</td>\n",
       "      <td>ComPO: Preference Alignment via Comparison Oracles</td>\n",
       "      <td>Direct alignment methods are increasingly used for aligning large language\\nmodels (LLMs) with human preferences. However, these methods suffer from the\\nissues of verbosity and likelihood displacement, which can be driven by the\\nnoisy preference pairs that induce similar likelihood for preferred and\\ndispreferred responses. The contributions of this paper are two-fold. First, we\\npropose a new preference alignment method based on comparison oracles and\\nprovide the convergence guarantee for its basic scheme. Second, we improve our\\nmethod using some heuristics and conduct the experiments to demonstrate the\\nflexibility and compatibility of practical scheme in improving the performance\\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\\nthe effectiveness of our method as an alternative to addressing the limitations\\nof existing direct alignment methods. A highlight of our work is that we\\nevidence the importance of designing specialized methods for preference pairs\\nwith distinct likelihood margin, which complements the recent findings in\\n\\citet{Razin-2025-Unintentional}.</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05-08 17:45:44+00:00</td>\n",
       "      <td>SITE: towards Spatial Intelligence Thorough Evaluation</td>\n",
       "      <td>Spatial intelligence (SI) represents a cognitive ability encompassing the\\nvisualization, manipulation, and reasoning about spatial relationships,\\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\\nmulti-choice visual question-answering, designed to assess large\\nvision-language models' spatial intelligence across diverse visual modalities\\n(single-image, multi-image, and video) and SI factors (figural to environmental\\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\\nand dynamic). Our approach to curating the benchmark combines a bottom-up\\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\\nclassification systems in cognitive science, which prompt us to design two\\nnovel types of tasks about view-taking and dynamic scenes. Extensive\\nexperiments reveal that leading models fall behind human experts especially in\\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\\npositive correlation between a model's spatial reasoning proficiency and its\\nperformance on an embodied AI task.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-05-08 17:44:45+00:00</td>\n",
       "      <td>Conversational Process Model Redesign</td>\n",
       "      <td>With the recent success of large language models (LLMs), the idea of\\nAI-augmented Business Process Management systems is becoming more feasible. One\\nof their essential characteristics is the ability to be conversationally\\nactionable, allowing humans to interact with the LLM effectively to perform\\ncrucial process life cycle tasks such as process model design and redesign.\\nHowever, most current research focuses on single-prompt execution and\\nevaluation of results, rather than on continuous interaction between the user\\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\\nempower domain experts in the creation and redesign of process models in an\\niterative and effective way. The proposed conversational process model redesign\\n(CPD) approach receives as input a process model and a redesign request by the\\nuser in natural language. Instead of just letting the LLM make changes, the LLM\\nis employed to (a) identify process change patterns from literature, (b)\\nre-phrase the change request to be aligned with an expected wording for the\\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\\nthe change to the process model. This multi-step approach allows for\\nexplainable and reproducible changes. In order to ensure the feasibility of the\\nCPD approach, and to find out how well the patterns from literature can be\\nhandled by the LLM, we performed an extensive evaluation. The results show that\\nsome patterns are hard to understand by LLMs and by users. Within the scope of\\nthe study, we demonstrated that users need support to describe the changes\\nclearly. Overall the evaluation shows that the LLMs can handle most changes\\nwell according to a set of completeness and correctness criteria.</td>\n",
       "      <td>[cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-05-08 17:43:35+00:00</td>\n",
       "      <td>RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles</td>\n",
       "      <td>Machine learning has become a powerful tool for enhancing data assimilation.\\nWhile supervised learning remains the standard method, reinforcement learning\\n(RL) offers unique advantages through its sequential decision-making framework,\\nwhich naturally fits the iterative nature of data assimilation by dynamically\\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\\nRL-based method that enhances data assimilation with physical constraints\\nthrough three key aspects. First, RL-DAUNCE inherits the computational\\nefficiency of machine learning while it uniquely structures its agents to\\nmirror ensemble members in conventional data assimilation methods. Second,\\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\\nensemble-as-agents design facilitates the enforcement of physical constraints\\nduring the assimilation process, which is crucial to improving the state\\nestimation and subsequent forecasting. A primal-dual optimization strategy is\\ndeveloped to enforce constraints, which dynamically penalizes the reward\\nfunction to ensure constraint satisfaction throughout the learning process.\\nAlso, state variable bounds are respected by constraining the RL action space.\\nTogether, these features ensure physical consistency without sacrificing\\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\\nensemble Kalman filter (EnKF), which fails catastrophically due to the\\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\\nof constrained EnKF, particularly in recovering intermittent signals, capturing\\nextreme events, and quantifying uncertainties, while requiring substantially\\nless computational effort.</td>\n",
       "      <td>[cs.LG, math-ph, math.MP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-05-08 17:36:36+00:00</td>\n",
       "      <td>clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations</td>\n",
       "      <td>The emergence of instruction-tuned large language models (LLMs) has advanced\\nthe field of dialogue systems, enabling both realistic user simulations and\\nrobust multi-turn conversational agents. However, existing research often\\nevaluates these components in isolation-either focusing on a single user\\nsimulator or a specific system design-limiting the generalisability of insights\\nacross architectures and configurations. In this work, we propose clem todd\\n(chat-optimized LLMs for task-oriented dialogue systems development), a\\nflexible framework for systematically evaluating dialogue systems under\\nconsistent conditions. clem todd enables detailed benchmarking across\\ncombinations of user simulators and dialogue systems, whether existing models\\nfrom literature or newly developed ones. It supports plug-and-play integration\\nand ensures uniform datasets, evaluation metrics, and computational\\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\\ntask-oriented dialogue systems within this unified setup and integrating three\\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\\nprovide actionable insights into how architecture, scale, and prompting\\nstrategies affect dialogue performance, offering practical guidance for\\nbuilding efficient and effective conversational AI systems.</td>\n",
       "      <td>[cs.CL]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2025-05-08 17:59:54+00:00   \n",
       "1 2025-05-08 17:59:47+00:00   \n",
       "2 2025-05-08 17:58:49+00:00   \n",
       "3 2025-05-08 17:58:45+00:00   \n",
       "4 2025-05-08 17:57:40+00:00   \n",
       "5 2025-05-08 17:56:57+00:00   \n",
       "6 2025-05-08 17:45:44+00:00   \n",
       "7 2025-05-08 17:44:45+00:00   \n",
       "8 2025-05-08 17:43:35+00:00   \n",
       "9 2025-05-08 17:36:36+00:00   \n",
       "\n",
       "                                                                                                            title  \\\n",
       "0                                                                                   3D Scene Generation: A Survey   \n",
       "1                             DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion   \n",
       "2                              Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning   \n",
       "3                                                          Flow-GRPO: Training Flow Matching Models via Online RL   \n",
       "4              StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant   \n",
       "5                                                              ComPO: Preference Alignment via Comparison Oracles   \n",
       "6                                                          SITE: towards Spatial Intelligence Thorough Evaluation   \n",
       "7                                                                           Conversational Process Model Redesign   \n",
       "8         RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles   \n",
       "9  clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                                      3D scene generation seeks to synthesize spatially structured, semantically\\nmeaningful, and photorealistic environments for applications such as immersive\\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\\nprocedural rules offered scalability but limited diversity. Recent advances in\\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\\ndistributions, improving fidelity, diversity, and view consistency. Recent\\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\\nreframing generation as image or video synthesis problems. This survey provides\\na systematic overview of state-of-the-art approaches, organizing them into four\\nparadigms: procedural generation, neural 3D-based generation, image-based\\ngeneration, and video-based generation. We analyze their technical foundations,\\ntrade-offs, and representative results, and review commonly used datasets,\\nevaluation protocols, and downstream applications. We conclude by discussing\\nkey challenges in generation capacity, 3D representation, data and annotations,\\nand evaluation, and outline promising directions including higher fidelity,\\nphysics-aware and interactive generation, and unified perception-generation\\nmodels. This review organizes recent advances in 3D scene generation and\\nhighlights promising directions at the intersection of generative AI, 3D\\nvision, and embodied intelligence. To track ongoing developments, we maintain\\nan up-to-date project page:\\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Current Structure-from-Motion (SfM) methods typically follow a two-stage\\npipeline, combining learned or geometric pairwise reasoning with a subsequent\\nglobal optimization step. In contrast, we propose a data-driven multi-view\\nreasoning approach that directly infers 3D scene geometry and camera poses from\\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\\nand cameras as pixel-wise ray origins and endpoints in a global frame and\\nemploys a transformer-based denoising diffusion model to predict them from\\nmulti-view inputs. To address practical challenges in training diffusion models\\nwith missing data and unbounded scene coordinates, we introduce specialized\\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\\nboth synthetic and real datasets, demonstrating that it outperforms classical\\nand learning-based approaches while naturally modeling uncertainty.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Leveraging current legal standards, we define bias through the lens of\\nmarginal benefits and objective testing with the novel metric \"Objective\\nFairness Index\". This index combines the contextual nuances of objective\\ntesting with metric stability, providing a legally consistent and reliable\\nmeasure. Utilizing the Objective Fairness Index, we provide fresh insights into\\nsensitive machine learning applications, such as COMPAS (recidivism\\nprediction), highlighting the metric's practical and theoretical significance.\\nThe Objective Fairness Index allows one to differentiate between discriminatory\\ntests and systemic disparities.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We propose Flow-GRPO, the first method integrating online reinforcement\\nlearning (RL) into flow matching models. Our approach uses two key strategies:\\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\\n(SDE) that matches the original model's marginal distribution at all timesteps,\\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\\nstrategy that reduces training denoising steps while retaining the original\\ninference timestep number, significantly improving sampling efficiency without\\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\\nperfect object counts, spatial relations, and fine-grained attributes, boosting\\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\\nFlow-GRPO also achieves substantial gains in human preference alignment.\\nNotably, little to no reward hacking occurred, meaning rewards did not increase\\nat the cost of image quality or diversity, and both remained stable in our\\nexperiments.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We present StreamBridge, a simple yet effective framework that seamlessly\\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\\nfundamental challenges in adapting existing models into online scenarios: (1)\\nlimited capability for multi-turn real-time understanding, and (2) lack of\\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\\nmemory buffer combined with a round-decayed compression strategy, supporting\\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\\nactivation model that can be effortlessly integrated into existing Video-LLMs,\\nenabling continuous proactive responses. To further support StreamBridge, we\\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\\nunderstanding, featuring interleaved video-text sequences and diverse\\ninstruction formats. Extensive experiments show that StreamBridge significantly\\nimproves the streaming understanding capabilities of offline Video-LLMs across\\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\\nstandard video understanding benchmarks.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Direct alignment methods are increasingly used for aligning large language\\nmodels (LLMs) with human preferences. However, these methods suffer from the\\nissues of verbosity and likelihood displacement, which can be driven by the\\nnoisy preference pairs that induce similar likelihood for preferred and\\ndispreferred responses. The contributions of this paper are two-fold. First, we\\npropose a new preference alignment method based on comparison oracles and\\nprovide the convergence guarantee for its basic scheme. Second, we improve our\\nmethod using some heuristics and conduct the experiments to demonstrate the\\nflexibility and compatibility of practical scheme in improving the performance\\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\\nthe effectiveness of our method as an alternative to addressing the limitations\\nof existing direct alignment methods. A highlight of our work is that we\\nevidence the importance of designing specialized methods for preference pairs\\nwith distinct likelihood margin, which complements the recent findings in\\n\\citet{Razin-2025-Unintentional}.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Spatial intelligence (SI) represents a cognitive ability encompassing the\\nvisualization, manipulation, and reasoning about spatial relationships,\\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\\nmulti-choice visual question-answering, designed to assess large\\nvision-language models' spatial intelligence across diverse visual modalities\\n(single-image, multi-image, and video) and SI factors (figural to environmental\\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\\nand dynamic). Our approach to curating the benchmark combines a bottom-up\\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\\nclassification systems in cognitive science, which prompt us to design two\\nnovel types of tasks about view-taking and dynamic scenes. Extensive\\nexperiments reveal that leading models fall behind human experts especially in\\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\\npositive correlation between a model's spatial reasoning proficiency and its\\nperformance on an embodied AI task.   \n",
       "7                                                                                                                                                                                        With the recent success of large language models (LLMs), the idea of\\nAI-augmented Business Process Management systems is becoming more feasible. One\\nof their essential characteristics is the ability to be conversationally\\nactionable, allowing humans to interact with the LLM effectively to perform\\ncrucial process life cycle tasks such as process model design and redesign.\\nHowever, most current research focuses on single-prompt execution and\\nevaluation of results, rather than on continuous interaction between the user\\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\\nempower domain experts in the creation and redesign of process models in an\\niterative and effective way. The proposed conversational process model redesign\\n(CPD) approach receives as input a process model and a redesign request by the\\nuser in natural language. Instead of just letting the LLM make changes, the LLM\\nis employed to (a) identify process change patterns from literature, (b)\\nre-phrase the change request to be aligned with an expected wording for the\\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\\nthe change to the process model. This multi-step approach allows for\\nexplainable and reproducible changes. In order to ensure the feasibility of the\\nCPD approach, and to find out how well the patterns from literature can be\\nhandled by the LLM, we performed an extensive evaluation. The results show that\\nsome patterns are hard to understand by LLMs and by users. Within the scope of\\nthe study, we demonstrated that users need support to describe the changes\\nclearly. Overall the evaluation shows that the LLMs can handle most changes\\nwell according to a set of completeness and correctness criteria.   \n",
       "8  Machine learning has become a powerful tool for enhancing data assimilation.\\nWhile supervised learning remains the standard method, reinforcement learning\\n(RL) offers unique advantages through its sequential decision-making framework,\\nwhich naturally fits the iterative nature of data assimilation by dynamically\\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\\nRL-based method that enhances data assimilation with physical constraints\\nthrough three key aspects. First, RL-DAUNCE inherits the computational\\nefficiency of machine learning while it uniquely structures its agents to\\nmirror ensemble members in conventional data assimilation methods. Second,\\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\\nensemble-as-agents design facilitates the enforcement of physical constraints\\nduring the assimilation process, which is crucial to improving the state\\nestimation and subsequent forecasting. A primal-dual optimization strategy is\\ndeveloped to enforce constraints, which dynamically penalizes the reward\\nfunction to ensure constraint satisfaction throughout the learning process.\\nAlso, state variable bounds are respected by constraining the RL action space.\\nTogether, these features ensure physical consistency without sacrificing\\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\\nensemble Kalman filter (EnKF), which fails catastrophically due to the\\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\\nof constrained EnKF, particularly in recovering intermittent signals, capturing\\nextreme events, and quantifying uncertainties, while requiring substantially\\nless computational effort.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The emergence of instruction-tuned large language models (LLMs) has advanced\\nthe field of dialogue systems, enabling both realistic user simulations and\\nrobust multi-turn conversational agents. However, existing research often\\nevaluates these components in isolation-either focusing on a single user\\nsimulator or a specific system design-limiting the generalisability of insights\\nacross architectures and configurations. In this work, we propose clem todd\\n(chat-optimized LLMs for task-oriented dialogue systems development), a\\nflexible framework for systematically evaluating dialogue systems under\\nconsistent conditions. clem todd enables detailed benchmarking across\\ncombinations of user simulators and dialogue systems, whether existing models\\nfrom literature or newly developed ones. It supports plug-and-play integration\\nand ensures uniform datasets, evaluation metrics, and computational\\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\\ntask-oriented dialogue systems within this unified setup and integrating three\\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\\nprovide actionable insights into how architecture, scale, and prompting\\nstrategies affect dialogue performance, offering practical guidance for\\nbuilding efficient and effective conversational AI systems.   \n",
       "\n",
       "                  categories  \n",
       "0                    [cs.CV]  \n",
       "1                    [cs.CV]  \n",
       "2             [cs.CY, cs.LG]  \n",
       "3             [cs.CV, cs.AI]  \n",
       "4      [cs.CV, cs.AI, cs.CL]  \n",
       "5      [cs.CL, cs.AI, cs.LG]  \n",
       "6                    [cs.CV]  \n",
       "7                    [cs.AI]  \n",
       "8  [cs.LG, math-ph, math.MP]  \n",
       "9                    [cs.CL]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'ai OR artificial intelligence OR machine learning'\n",
    "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "\n",
    "papers = []\n",
    "for result in search.results():\n",
    "    papers.append({\n",
    "        'published': result.published,\n",
    "        'title': result.title,\n",
    "        'abstract': result.summary,\n",
    "        'categories': result.categories\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Example abstract from API\n",
    "abstract = df['abstract'][0]\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summarization_result = summarizer(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3D scene generation seeks to synthesize spatially structured, semanticallymeaningful, and photorealistic environments. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models and 3D representations have enabled the learning of real-world scene distributions.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
