{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  9 17:06:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   44C    P4              9W /   55W |     708MiB /   8188MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3584      G   /usr/lib/xorg/Xorg                            346MiB |\n",
      "|    0   N/A  N/A      3959      G   /usr/bin/gnome-shell                           47MiB |\n",
      "|    0   N/A  N/A      5095      G   ...68,262144 --variations-seed-version         98MiB |\n",
      "|    0   N/A  N/A      7862      G   ...erProcess --variations-seed-version        158MiB |\n",
      "|    0   N/A  N/A     14171      G   /usr/bin/gnome-text-editor                     42MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "'''\n",
    "1. Text Classification: Assigning a category to a pice of text.\n",
    "Sentiment Analysis\n",
    "Topic Classification\n",
    "Spam Detection\n",
    "'''\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "\n",
    "'''\n",
    "2. Token Classification: Assigning labels to individual tokens in a sequence.\n",
    "Named Entity Recognition (NER)\n",
    "Part-of-Speech Tagging\n",
    "'''\n",
    "\n",
    "token_classifier = pipeline('token-classification')\n",
    "\n",
    "'''\n",
    "3. Question Answering: Extracting an answer from a given context based on a question.\n",
    "'''\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "'''\n",
    "4. Text Generation: Generating text based on a given prompt.\n",
    "Language Modeling\n",
    "Story Generation\n",
    "\n",
    "'''\n",
    "\n",
    "text_generator = pipeline('text-generation')\n",
    "\n",
    "'''\n",
    "5. Summarization: Condensing a longer text into a shorter summary.\n",
    "'''\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.999748170375824}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"I was not happy with the last Mission Impossible Movie\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995480179786682}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis')(\"I was confused with the Barie Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9924917817115784}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis')\\\n",
    "    (\"Everyday lots of LLMs paper are published about LLMs Evaluation. \\\n",
    "    Lots of them Looks very Promising \\\n",
    "    I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "    There is still lots to do. \\\n",
    "    Don't you think?\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.840279757976532}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = 'sentiment-analysis', model = 'facebook/bart-large-mnli')\\\n",
    "                                    (\"Everyday lots of LLMs papers are published about LLMs Evaluation. \\\n",
    "                                    Lots of them Looks very Promising. \\\n",
    "                                    I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "                                    There is still lots to do. \\\n",
    "                                    Don't you think?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9978686571121216}\n",
      "{'label': 'NEGATIVE', 'score': 0.9995476603507996}\n",
      "{'label': 'NEGATIVE', 'score': 0.9983084201812744}\n",
      "{'label': 'NEGATIVE', 'score': 0.9960630536079407}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task = 'sentiment-analysis')\n",
    "\n",
    "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
    "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
    "            \"PassiveAgressive is the name of a Linear Regression model that so many people do not know.\", \\\n",
    "            \"I hate long Meetings\"]\n",
    "\n",
    "results = classifier(task_list)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'admiration', 'score': 0.7406538128852844}\n",
      "{'label': 'confusion', 'score': 0.9066852331161499}\n",
      "{'label': 'neutral', 'score': 0.79802405834198}\n",
      "{'label': 'anger', 'score': 0.7903208136558533}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task = \"sentiment-analysis\", model = \"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
    "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
    "            \"PassiveAgressive is the name of a Linear Regression model that so many people do not know.\", \\\n",
    "            \"I hate long Meetings\"]\n",
    "\n",
    "results = classifier(task_list)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Today is a rainy day in London. This day has been quite an experience. A lot of people have left all day at dawn, and there is no time for it at night. I've been in London the morning before until 12am, so\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline('text-generation', model=\"distilbert/distilgpt2\")\n",
    "generated_text = text_generator(\"Today is a rainy day in London\", \\\n",
    "                                truncation=True,\n",
    "                                num_return_sequences=2)\n",
    "\n",
    "print(\"Generated Text: \", generated_text[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7823826670646667,\n",
       " 'start': 5,\n",
       " 'end': 25,\n",
       " 'answer': 'developing AI models'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline('question-answering')\n",
    "question = \"What is my job?\"\n",
    "context = \"I am developing AI models with Python.\"\n",
    "\n",
    "qa_model(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '2 stars', 'score': 0.5077593326568604}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\"\"\"\n",
    "model_name1 = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "mytokenizer1 = DistilBertTokenizer.from_pretrained(model_name1)\n",
    "mymodel1 = DistilBertForSequenceClassification.from_pretrained(model_name1)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", )\n",
    "res = classifier(\"I was so not happy with the Barie Movie\")\n",
    "print(res)\n",
    "\"\"\"\n",
    "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "mymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
    "mytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=mymodel2, tokenizer=mytokenizer2)\n",
    "res = classifier(\"I was so not happy with the Barie Movie\")\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"I was so not happy with the Barbie Movie\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the tokenized text and IDs\n",
    "print(\"Tokenized Text: \", tokens)\n",
    "print(\"Token IDs: \", input_ids)\n",
    "\n",
    "# Encode the text (tokenization + converting to input IDs)\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"Encoded Inpit:\", encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
